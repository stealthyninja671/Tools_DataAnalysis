{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b4ce8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "\n",
    "#Package Imports\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "#import IProgress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#import sklearn\n",
    "#from tqdm import tqdm\n",
    "#from tqdm import *\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import shelve\n",
    "import string\n",
    "import lmfit\n",
    "\n",
    "#pyfai --> Function to use if I want to write new functions to read in the 2D images and create\n",
    "# the 1D profiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be233147",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2ea87",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6d0bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(x1,y1,x2,y2):\n",
    "    m = (y2-y1)/(x2-x1)\n",
    "    return m\n",
    "\n",
    "def intercept(x1,y1,x2,y2):\n",
    "    m = slope(x1,y1,x2,y2)\n",
    "    b = y2-(m*x2)\n",
    "    return b\n",
    "\n",
    "def interpol_lin(x1,y1,x2,y2,x):\n",
    "    m = slope(x1,y1,x2,y2)\n",
    "    y = m*(x-x1) + y1\n",
    "    return y\n",
    "\n",
    "#Returns the Index of the closest value in a column\n",
    "def closest(df,colx,value,offset=0):\n",
    "    #NOTE colx is to be a string identifying the \n",
    "    #dataframe columns with the respective data\n",
    "    closest_index = 0\n",
    "    minimum = df[colx].max() - df[colx].min()\n",
    "    for index in range(len(df)):\n",
    "        difference = abs(df[colx][index]-value)\n",
    "        #print(\"Current Index: \" + str(index) + \";  Current Dif: \" + str(difference)+\";  Minimum: \" + str(minimum))\n",
    "        if difference<minimum:\n",
    "            closest_index = index\n",
    "            minimum = difference\n",
    "    #NOTE This function will choose the lower index value in the event\n",
    "    #there are multiple equally close points--> Adjusted now \n",
    "    return closest_index+offset\n",
    "\n",
    "#returns the Index immediately before the closest value in a column\n",
    "#Function can be done more simply with a basic > or < check if the list is assumed to be ordered\n",
    "def closest_ordered(df,colx,value,offset=0):\n",
    "    #NOTE colx is to be a string identifying the \n",
    "    #dataframe columns with the respective data\n",
    "    closest_index = 0\n",
    "    minimum = df[colx].max() - df[colx].min()\n",
    "    for index in range(len(df)):\n",
    "        difference = abs(df.iloc[index][colx]-value)\n",
    "        #print(\"Current Index: \" + str(index) + \";  Current Dif: \" + str(difference)+\";  Minimum: \" + str(minimum))\n",
    "        if difference<minimum:\n",
    "            closest_index = index\n",
    "            minimum = difference\n",
    "    #NOTE This function will choose the lower index value in the event\n",
    "    #there are multiple equally close points--> Adjusted now \n",
    "    if df[colx][closest_index] > value:\n",
    "        closest_index -= 1   \n",
    "    return closest_index+offset\n",
    "\n",
    "def correction_slopematch(rawx,rawy,dilation,slide=0,direction = 1):\n",
    "    return (rawy*(dilation**(direction))) - slide\n",
    "    #return (rawy*(dilation**(direction)))\n",
    "#-1 refers to a scale DOWN\n",
    "#IMPORTANT: When using with df.apply() the FIRST variable is the only one that\n",
    "#can be iterated over\n",
    "\n",
    "def correction_slopematch_log(rawx,rawy,dilation,slide=0,direction = 1):\n",
    "    return (rawy**(dilation*direction))*(10**(slide*direction))\n",
    "    #return (rawy**(dilation*direction))\n",
    "#-1 refers to a scale DOWN\n",
    "#IMPORTANT: When using with df.apply() the FIRST variable is the only one that\n",
    "#can be iterated over\n",
    "\n",
    "def correction_valuematch(rawx,rawy,slope,intercept=0,direction = 1):\n",
    "    return rawy*(slope**(direction)) \n",
    "#IMPORTANT: When using with df.apply() the FIRST variable is the only one that\n",
    "#can be iterated over\n",
    "\n",
    "def fraction_symm(x,lowerbound,upperbound):\n",
    "    #Weight Starts at 0 the closer it is to the lowerbound\n",
    "    if lowerbound == upperbound:\n",
    "        return 0.0 \n",
    "    elif x < lowerbound:\n",
    "        return 0.0\n",
    "    elif x > upperbound:\n",
    "        return 1.0\n",
    "    else: \n",
    "        weight = ((x-lowerbound)/(upperbound-lowerbound))\n",
    "        return weight\n",
    "    \n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "#Function to add a value to a dictionary --> To be Defunct soon\n",
    "#def sample_metadata(category,entry,name):\n",
    "#    if not (category in fields):\n",
    "#        raise Exception(\"The category you've chosen is not in the established set of fields\")\n",
    "#    name = {}\n",
    "#    return new\n",
    "\n",
    "def closest_list_value(list,value):\n",
    "    arr = np.asarray(list)\n",
    "    i = (np.abs(arr - value)).argmin()\n",
    "    return arr[i]\n",
    "\n",
    "def closest_list_index(list,value):\n",
    "    arr = np.asarray(list)\n",
    "    i = (np.abs(arr - value)).argmin()\n",
    "    return i\n",
    "\n",
    "def check_positive(df):\n",
    "    df=df[df.select_dtypes(include=[np.number]).ge(0).all(1)]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c42cc",
   "metadata": {},
   "source": [
    "## Utility Functions: Autosave; Backups etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34effba7",
   "metadata": {},
   "source": [
    "### Save and Autosave Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf4f88",
   "metadata": {},
   "source": [
    "### add_dat_extension & process_directory: Directory extension and name modifiers\n",
    "Adds dat extension to naked files without an extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59ba976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dat_extension(filename):\n",
    "    #print(\"Function 1 Triggered\")\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if not ext:\n",
    "        new_filename = f\"{name}.dat\"\n",
    "        return new_filename\n",
    "    return filename\n",
    "\n",
    "def process_directory(root):\n",
    "    #print(\"Function 2 Triggered\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        #print(dirpath)\n",
    "        #print(dirnames)\n",
    "        #print(filenames)\n",
    "        #print(\"Next Sample\")\n",
    "        for filename in filenames:\n",
    "            if filename != \".DS_Store\":\n",
    "                old_filepath = os.path.join(dirpath, filename)\n",
    "                new_filename = add_dat_extension(filename)\n",
    "                new_filepath = os.path.join(dirpath, new_filename)\n",
    "\n",
    "                if filename != new_filename:\n",
    "                    os.rename(old_filepath, new_filepath)\n",
    "                    print(f\"Renamed: {old_filepath} -> {new_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0667a",
   "metadata": {},
   "source": [
    "## Data Ingestion: Data import, export, and Metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3cd45",
   "metadata": {},
   "source": [
    "### Import Data Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fddd3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_dat(name, folder, prefix, suffix,independent=\"q\",dependent=\"Intensity\"):    \n",
    "    #Depending on the output panel, sample .dat files may have a different prefix than the sample name\n",
    "    \n",
    "    if not (folder[-1] == \"/\"):\n",
    "        raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    "    length_suffix = len(suffix)\n",
    "    length_prefix = len(prefix)\n",
    "    length_name = len(name) #Not currently used value --> Meaning I can put whatever information I want here\n",
    "    \n",
    "    #Initial Copy of original Code pt2\n",
    "    # assign path\n",
    "    path, dirs, files = next(os.walk(folder))\n",
    "    files.sort()\n",
    "    #print(\"Data Import:\")\n",
    "    #print(files)\n",
    "    #print()\n",
    "    file_count = len(files)\n",
    "    # create empty list\n",
    "    f_dataframes = []\n",
    "    #print(\"There are \"+str(file_count)+\" files in this directory\")\n",
    "  \n",
    "    # append datasets to the list \n",
    "    for i in range(file_count):\n",
    "        if files[i][(-1*length_suffix):] == suffix:\n",
    "            #print(folder+files[i])\n",
    "            temp_df = pd.read_csv(folder+files[i], comment='#',sep='\\t',usecols = [0,1], names = [independent,dependent])\n",
    "            f_dataframes.append(temp_df)\n",
    "        else: \n",
    "            continue\n",
    "    if len(f_dataframes) == 0:\n",
    "        raise Exception(\"Looks like your suffix was not found in the folder; please check and try again\")\n",
    "    \n",
    "    return f_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a62ca0",
   "metadata": {},
   "source": [
    "### Create Energies List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59475571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_energieslist(name, folder, prefix, suffix):\n",
    "    #Depending on the output panel, sample .dat files may have a different prefix than the sample name\n",
    "    length_suffix = len(suffix)\n",
    "    length_prefix = len(prefix)\n",
    "    length_name = len(name)\n",
    "    \n",
    "    path, dirs, files = next(os.walk(folder))\n",
    "    files.sort()\n",
    "    #print('Energies Import:')\n",
    "    #print(files)\n",
    "    #print()\n",
    "    #print(os.listdir(folder))\n",
    "    file_count = len(files)\n",
    "    f_energies_list_str = []\n",
    "    sorteddirs = sorted(os.listdir(folder))\n",
    "    for i in range(file_count):\n",
    "        if files[i][(-1*length_suffix):] == suffix:\n",
    "            #tempname = os.listdir(folder)[i][(length_prefix):(-1*(length_suffix))]\n",
    "            tempname = sorteddirs[i][(length_prefix):(-1*(length_suffix))]\n",
    "            #print(tempname)\n",
    "            underscorefree = \"\"\n",
    "            index = 0\n",
    "            while index < len(tempname):\n",
    "                if tempname[index:(index+2)] == \"_0\": \n",
    "                    index += 2\n",
    "                    continue\n",
    "                elif tempname[index] == \"_\":\n",
    "                    index += 1\n",
    "                    continue \n",
    "                else: \n",
    "                    underscorefree = underscorefree + tempname[index]\n",
    "                    index += 1\n",
    "            f_energies_list_str.append(underscorefree)\n",
    "        else:\n",
    "            continue       \n",
    "        \n",
    "    if len(f_energies_list_str) == 0:\n",
    "        raise Exception(\"Looks like your suffix was not found in the folder; please check and try again\")\n",
    "\n",
    "    f_energies_list = [float(value) for value in f_energies_list_str]\n",
    "    #print(energies_list)\n",
    "    #Creates Key Lists:\n",
    "    #energies_list --> A list of all the energies taken for a given sample, read in from file name\n",
    "    #dataframes_list --> The list of all the dataframes, each dataframe constitutes one energy. \n",
    "    return f_energies_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd521c",
   "metadata": {},
   "source": [
    "### Import: all folders for a given SET of samples (Scattering & OPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cc88bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_set(folder, pre, suf,ind=\"q\",dep=\"Intensity\"):\n",
    "    #REQUIRES CONSISTENT PREFIX AND SUFFIX\n",
    "    names = []\n",
    "    names_e = []\n",
    "    \n",
    "    #Error Checking\n",
    "    if not (folder[-1] == \"/\"):\n",
    "        raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    " \n",
    "    \n",
    "    #Loop to check root folders for Sample dirs and for files\n",
    "    #for index in range(len(files)):\n",
    "    #    import_data_dat(files[index],folder+files[index],prefix,suffix,ind,dep)\n",
    "    #    import_energies(files[index],folder+files[index],prefix,suffix)\n",
    "    \n",
    "    path1, dirs1, files1 = next(os.walk(folder))\n",
    "\n",
    "    #cycle into the directories within folder\n",
    "    for index1 in range(len(dirs1)):\n",
    "        path2,dirs2,files2 = next(os.walk(folder+\"/\"+dirs1[index1]))\n",
    "        #Check the CCD0/CCD100 folders --> Final import \n",
    "        for index2 in range(len(dirs2)):\n",
    "            temp_name = dirs1[index1]+\"_\"+dirs2[index2]\n",
    "            temp_dir = path1 + dirs1[index1]+\"/\" +dirs2[index2]+\"/\"\n",
    "            temp_name_e = temp_name + \"_energies\"\n",
    "            t_glob = globals()\n",
    "            t_glob.__setitem__(temp_name,import_data_dat(temp_name,temp_dir,pre,suf,ind,dep))\n",
    "            t_glob.__setitem__(temp_name_e,import_energieslist(temp_name_e,temp_dir,pre,suf))\n",
    "            names.append(temp_name)\n",
    "            names_e.append(temp_name_e)\n",
    "    print(names)\n",
    "    #print(names_e)\n",
    "    return (names,names_e)\n",
    "\n",
    "#import_data_dat(name, folder, prefix, suffix,independent=\"q\",dependent=\"Intensity\")\n",
    "#import_energieslist(name, folder, prefix, suffix):\n",
    "#re.sub(r'[^\\w]', ' ', <string>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d885f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f143aa90",
   "metadata": {},
   "source": [
    "### Export one list of dataframes and an energies list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b62af4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##accepts data and energies as LISTS \n",
    "def export_csv(name,data,energies,path=\"\"):\n",
    "    if not len(data) == len(energies):\n",
    "        raise Exception(\"Data and Metadata lists are not the same length\")\n",
    "    path=\"Data_Export/\"+path+name+\"/\"\n",
    "    filepath = Path(path)\n",
    "    filepath.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for index in range(len(data)):\n",
    "        tempname = name + \"_\" + str(energies[index]) + \".csv\"\n",
    "        #print(path)\n",
    "        #filepath = Path(path)\n",
    "        #data[index].to_csv(filepath/name/str(energies[index]))\n",
    "        data[index].to_csv(filepath/tempname, index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca09f0",
   "metadata": {},
   "source": [
    "### Build blank sample dictionary: metadata_blanksample\n",
    "Saves the name and whatever added data to a sample dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cca1172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_blanksample(name,date=\"\",description = \"\",trial=\"\",viable=\"\",material=\"\",concentration=\"\", spinspeed=\"\",solvent=\"\",anneal_temp=\"\",anneal_time=\"\",additive_material=\"\",additive_amount=\"\",ccd0_data=\"\",ccd0_energies=\"\",ccd100_data=\"\",ccd100_energies=\"\",stitched_data=\"\",stitched_energies=\"\"):\n",
    "        #Error type checking\n",
    "        if not type(name)== \"string\":\n",
    "                raise Exception(\"sample name must be a string\")\n",
    "\n",
    "    \n",
    "        f_blanksample =                 \\\n",
    "                {\"name\":name,             \\\n",
    "                \"descr\":description,     \\\n",
    "                \"date\":date,              \\\n",
    "                \"trial\":trial,             \\\n",
    "                \"viable\":viable,            \\\n",
    "                \"material\":material,          \\\n",
    "                \"concentration\":concentration,      \\\n",
    "                \"spinspeed\":spinspeed,          \\\n",
    "                \"solvent\":solvent,            \\\n",
    "                \"anneal\":                       \\\n",
    "                        {\"temperature\":anneal_temp,      \\\n",
    "                        \"time\":anneal_time},              \\\n",
    "                \"additive\":                     \\\n",
    "                        {\"material\":additive_material,         \\\n",
    "                        \"amount%\":additive_amount},           \\\n",
    "                \"ccd0\":                         \\\n",
    "                        {\"data\":ccd0_data,             \\\n",
    "                        \"energies\":ccd0_energies},          \\\n",
    "                \"ccd100\":                       \\\n",
    "                        {\"data\":ccd100_data,             \\\n",
    "                        \"energies\":ccd100_energies},          \\\n",
    "                \"stitched\":                     \\\n",
    "                        {\"data\":stitched_data,             \\\n",
    "                        \"energies\":stitched_energies},          \\\n",
    "                \n",
    "                }\n",
    "\n",
    "        #Establish Variable name:\n",
    "        t_glob = globals()\n",
    "        t_glob.__setitem__(name,f_blanksample)\n",
    "\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea79f4",
   "metadata": {},
   "source": [
    "## Data Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e3a08",
   "metadata": {},
   "source": [
    "### Plot All: plot_allenergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b88b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_allenergies(f_name,f_data,f_energy,xscale = \"log\",yscale=\"log\",distr = \"index\",path=\"\"):\n",
    "    name = f_name + \": All Energies\"\n",
    "    path = \"Figures/\" + path\n",
    "\n",
    "    #Error Checking\n",
    "    if not (path[-1] == \"/\"):\n",
    "        raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    "    filepath = Path(path)\n",
    "    filepath.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    #Make First plot, standard intensity plot\n",
    "    fig1 = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_ylabel('Intensity', fontsize = 20)\n",
    "    ax1.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "    plt.ylim(bottom=10e-15,top=10e-7) #BANDAID SOLUTION for Value elimination\n",
    "\n",
    "    #DEFAULT: use linear scaling --> Use Log Scaling actually\n",
    "    ax1.set_xscale(xscale)\n",
    "    ax1.set_yscale(yscale)\n",
    "    \n",
    "    #Choose color distribution scheme --> DEFAULT: Scale by Index for better contrast\n",
    "    if distr== \"energy\":\n",
    "        norm = cl.Normalize(vmin=min(f_energy), vmax=max(f_energy))\n",
    "        cmap = matplotlib.cm.get_cmap('rainbow')\n",
    "        colors = [cmap(norm(float(f_energy[i]))) for i in range(len(f_energy))]\n",
    "    else:\n",
    "        cmap = matplotlib.cm.get_cmap('rainbow')\n",
    "        norm = cl.Normalize(vmin=0, vmax=len(f_energy))\n",
    "        colors = [cmap(norm(i)) for i in range(len(f_energy))]\n",
    "\n",
    "    for index in range(len(f_data)):\n",
    "        ax1.plot(f_data[index]['q'], f_data[index]['Intensity'],color=colors[index], label=f_energy[index])\n",
    "\n",
    "    leg1 = ax1.legend(ncol=4,loc='upper right')\n",
    "    \n",
    "    #Save the plot as png\n",
    "    plt.savefig(path+f_name + \"_allenergies_I-loglog.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    #Make second I*q^2 plot \n",
    "    fig2 = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    ax2.set_ylabel('$I*q^2$', fontsize = 20)\n",
    "    ax2.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    ax2.set_xscale('log')\n",
    "\n",
    "    #ax1.set_ylim([10e4, 10e9])\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "\n",
    "    #xscale is dependent on the python xscale API\n",
    "    #DEFAULT: use \"linear\" for scale\n",
    "    ax2.set_xscale(xscale)\n",
    "\n",
    "    #Choose color distribution scheme --> DEFAULT: Scale by Index for better contrast\n",
    "    if distr== \"energy\":\n",
    "        norm2 = cl.Normalize(vmin=min(f_energy), vmax=max(f_energy))\n",
    "        cmap2 = matplotlib.cm.get_cmap('rainbow')\n",
    "        colors2 = [cmap(norm(float(f_energy[i]))) for i in range(len(f_energy))]\n",
    "    else:\n",
    "        cmap2 = matplotlib.cm.get_cmap('rainbow')\n",
    "        norm2 = cl.Normalize(vmin=0, vmax=len(f_energy))\n",
    "        colors2 = [cmap(norm(i)) for i in range(len(f_energy))]\n",
    "\n",
    "    for index in range(len(f_data)):\n",
    "        #ax1.plot(dataframes_list[index])\n",
    "        ax2.plot(f_data[index]['q'], (f_data[index]['Intensity']*(f_data[index]['q']**2)),color=colors2[index], label=f_energy[index])\n",
    "\n",
    "    leg2 = ax2.legend(ncol=4,loc='upper right')\n",
    "    plt.savefig(path+f_name + \"_allenergies_Iq2-loglin.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa458117",
   "metadata": {},
   "source": [
    "### Plot Select Energies: plot_selectenergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6f075a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot \n",
    "def plot_selectenergy(f_name,f_data,f_energy,energies,xscale = \"log\",yscale=\"log\",distr = \"index\",path=\"\"):\n",
    "    path = \"Figures/\" + path\n",
    "\n",
    "    #Error Checking\n",
    "    if not (path[-1] == \"/\"):\n",
    "        raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    "    filepath = Path(path)\n",
    "    filepath.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    #FIRST PLOT\n",
    "    #Extract the indexes of the correct dataframes and add to list\n",
    "    indexes = []\n",
    "    for value in energies:\n",
    "        #indexes.append(f_energy.index(value))\n",
    "        indexes.append(closest_list_index(f_energy,value))\n",
    "     \n",
    "    name = f_name + \": Select Energies\"\n",
    "\n",
    "    #Create Plot Frame\n",
    "    figA = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "\n",
    "    axA = figA.add_subplot(111)\n",
    "    #plt.ylabel('Intensity')\n",
    "    #plt.xlabel('q')\n",
    "    #figA.supxlabel('q', fontsize = 20)\n",
    "    #figA.supylabel('Intensity', fontsize = 20)\n",
    "    axA.set_ylabel('Intensity', fontsize = 20)\n",
    "    axA.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    #ax2.set_ylim([10e4, 10e-25])\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    figA.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "    plt.ylim(bottom=10e-15,top=10e-7) #BANDAID SOLUTION for Value elimination\n",
    "\n",
    "    #plt.ginput(2)\n",
    "    \n",
    "    #xscale is dependent on the python xscale API\n",
    "    #DEFAULT: use \"linear\" for scale\n",
    "    axA.set_xscale(xscale)\n",
    "    axA.set_yscale(yscale)\n",
    "\n",
    "    #Choose color distribution scheme --> DEFAULT: Scale by Index for better contrast\n",
    "    if distr== \"energy\":\n",
    "        norm = cl.Normalize(vmin=min(f_energy), vmax=max(f_energy))\n",
    "        cmap = matplotlib.cm.get_cmap('rainbow')\n",
    "        colors = [cmap(norm(float(f_energy[i]))) for i in range(len(f_energy))]\n",
    "    else:\n",
    "        cmap = matplotlib.cm.get_cmap('rainbow')\n",
    "        norm = cl.Normalize(vmin=0, vmax=len(f_energy))\n",
    "        colors = [cmap(norm(i)) for i in range(len(f_energy))]\n",
    "\n",
    "\n",
    "    for index in indexes:\n",
    "        axA.plot(f_data[index]['q'], f_data[index]['Intensity'],color=colors[index], label=f_energy[index])\n",
    "\n",
    "    legA = axA.legend(ncol=4,loc='upper right',frameon = False, fontsize = 15)\n",
    "    plt.savefig(path+f_name + \"_selectenergies_I-loglog.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    #NOW THE Iq^2\n",
    "    #Create Plot Frame\n",
    "    figB = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "\n",
    "    axB = figB.add_subplot(111)\n",
    "    #plt.ylabel('Intensity')\n",
    "    #plt.xlabel('q')\n",
    "    #figB.supxlabel('q', fontsize = 20)\n",
    "    #figB.supylabel('Intensity', fontsize = 20)\n",
    "    axB.set_ylabel('$I*q^2$', fontsize = 20)\n",
    "    axB.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    axB.set_yscale('log')\n",
    "    #ax2.set_ylim([10e4, 10e9])\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    figB.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "    #plt.ginput(2)\n",
    "    \n",
    "    \n",
    "    #xscale is dependent on the python xscale API\n",
    "    #DEFAULT: use \"linear\" for scale\n",
    "    axB.set_xscale(xscale)\n",
    "\n",
    "    #Choose color distribution scheme --> DEFAULT: Scale by Index for better contrast\n",
    "    if distr== \"energy\":\n",
    "        norm2 = cl.Normalize(vmin=min(f_energy), vmax=max(f_energy))\n",
    "        cmap2 = matplotlib.cm.get_cmap('rainbow')\n",
    "        colors2 = [cmap(norm(float(f_energy[i]))) for i in range(len(f_energy))]\n",
    "    else:\n",
    "        cmap2 = matplotlib.cm.get_cmap('rainbow')\n",
    "        norm2 = cl.Normalize(vmin=0, vmax=len(f_energy))\n",
    "        colors2 = [cmap(norm(i)) for i in range(len(f_energy))]\n",
    "\n",
    "    for index in indexes:\n",
    "        #axB.plot(dataframes_list[index])\n",
    "        axB.plot(f_data[index]['q'], (f_data[index]['Intensity']*(f_data[index]['q']**2)),color=colors2[index], label=f_energy[index])\n",
    "\n",
    "    legB = axB.legend(ncol=4,loc='upper right',frameon = False, fontsize = 15)\n",
    "    plt.savefig(path+f_name + \"_selectenergies_Iq2-loglin.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e114fe",
   "metadata": {},
   "source": [
    "### Plot SINGLE Energy: plot_singleenergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52db2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot \n",
    "def plot_singleenergy(f_name,f_data,f_energy,energy,xscale = \"log\",yscale=\"log\",distr = \"index\",path=\"\",line_color='blue'):\n",
    "    path = \"Figures/\" + path\n",
    "\n",
    "    #Error Checking\n",
    "    if not (path[-1] == \"/\"):\n",
    "        raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    "    filepath = Path(path)\n",
    "    filepath.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    #FIRST PLOT\n",
    "    #Extract the indexes of the correct dataframes and add to list\n",
    "    index = closest_list_index(f_energy,energy)\n",
    "    energy = f_energy[index]\n",
    "     \n",
    "    name = f_name + \": \" + str(energy)+\" eV\"\n",
    "\n",
    "    #Create Plot Frame\n",
    "    figA = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "\n",
    "    axA = figA.add_subplot(111)\n",
    "    #plt.ylabel('Intensity')\n",
    "    #plt.xlabel('q')\n",
    "    #figA.supxlabel('q', fontsize = 20)\n",
    "    #figA.supylabel('Intensity', fontsize = 20)\n",
    "    axA.set_ylabel('Intensity', fontsize = 20)\n",
    "    axA.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    #ax2.set_ylim([10e4, 10e9])\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    figA.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "    plt.ylim(bottom=10e-15,top=10e-7) #BANDAID SOLUTION for Value elimination\n",
    "    #plt.ginput(2)\n",
    "    \n",
    "    #xscale is dependent on the python xscale API\n",
    "    #DEFAULT: use \"linear\" for scale\n",
    "    axA.set_xscale(xscale)\n",
    "    axA.set_yscale(yscale)\n",
    "\n",
    "    #Detached plot command from multiloop function\n",
    "    axA.plot(f_data[index]['q'], f_data[index]['Intensity'], label=f_energy[index],color = line_color)\n",
    "\n",
    "    #legA = axA.legend(ncol=4,loc='upper right',frameon = False, fontsize = 15)\n",
    "    plt.savefig(path+f_name + \"_singleenergy-\"+str(energy)+\"_I-loglog.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    #NOW THE Iq^2\n",
    "    #Create Plot Frame\n",
    "    figB = plt.figure(figsize=(20, 10),dpi = 200)\n",
    "\n",
    "    axB = figB.add_subplot(111)\n",
    "    #plt.ylabel('Intensity')\n",
    "    #plt.xlabel('q')\n",
    "    #figB.supxlabel('q', fontsize = 20)\n",
    "    #figB.supylabel('Intensity', fontsize = 20)\n",
    "    axB.set_ylabel('$I*q^2$', fontsize = 20)\n",
    "    axB.set_xlabel('q ($A^{-1}$)', fontsize = 20)\n",
    "    axB.set_yscale('log')\n",
    "    #ax2.set_ylim([10e4, 10e9])\n",
    "    plt.xticks(fontsize= 18)\n",
    "    plt.yticks(fontsize= 18)\n",
    "    figB.patch.set_facecolor('white')\n",
    "    plt.title(name, fontsize = 25, pad=15)\n",
    "    #plt.ginput(2)\n",
    "    \n",
    "    \n",
    "    #xscale is dependent on the python xscale API\n",
    "    #DEFAULT: use \"linear\" for scale\n",
    "    axB.set_xscale(xscale)\n",
    "\n",
    "    axB.plot(f_data[index]['q'], (f_data[index]['Intensity']*(f_data[index]['q']**2)),color=line_color, label=f_energy[index])\n",
    "\n",
    "    legB = axB.legend(ncol=4,loc='upper right',frameon = False, fontsize = 15)\n",
    "    plt.savefig(path+f_name + \"_singleenergy-\"+str(energy)+\"_Iq2-loglog.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c551df5",
   "metadata": {},
   "source": [
    "## Processing & Analysis Functions: Manipulate Data in Some Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d73cb",
   "metadata": {},
   "source": [
    "### Stitch WAXS-SAXS: analysis_stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "874f4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Stitching of CCD0 and CCD100 --> Intake; CCD0 data frame, CCD100 data frame, Full vs Windowed, \n",
    "# Upper Bound and Lower Bound\n",
    "# SMOOTHING accounts for the number of additional points added to the linear fit at the stich point\n",
    "# WINDOW considers how much of the data to use in the weighted averaging\n",
    "# SMOOTHING - The number of points included in the linear regression\n",
    "# REGIME -  How the Data is to be plotted: linlin and loglog\n",
    "# METHOD - Method of data matching: slope and value (Slope does slope matching and then a baseline correction)\n",
    "        # Value matching multiplies the whole ccd100 dataset to match at the stitch point\n",
    "\n",
    "\n",
    "def analysis_stitch(axisX,axisY,ccd100,ccd0,q=None,smoothing=25,window=1,regime=\"loglog\",method=\"slope\",debug = 0,checkpoint=0,progress=0):\n",
    "    #THIS PROGRAM WAS WRITTEN BACKWARDS:\n",
    "    #Originally i thought that CCD0 corresponded to lower q value, that is the opposite case\n",
    "    #Bandaid Solution: Swap ccd0 and ccd100 in the ingestion phase of the program (first line) --> IMPLEMENTED\n",
    "    \n",
    "    #Spot to manually activate debugging mode\n",
    "    #debug = 1 #Debugging Mode is on\n",
    "    #checkpoint = 1\n",
    "    #progress=1\n",
    "    \n",
    "    #make necessary copies of the dataframes so I dont end up modifying any\n",
    "    ccd0working = ccd0.copy()\n",
    "    ccd100working = ccd100.copy()\n",
    "\n",
    "    #Technique 3: Replace negatives tiny value using AxisY input\n",
    "    ccd0working.loc[ccd0working[axisY]<0,axisY]=1e-200\n",
    "    ccd100working.loc[ccd100working[axisY]<0,axisY]=1e-200\n",
    "\n",
    "    \n",
    "    if (debug == 1):\n",
    "        #DEBUG CODE: DELETE LATER\n",
    "        ax1 = ccd100.plot(x =axisX, y=axisY, kind = 'line')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_xscale('log')\n",
    "        ccd0.plot(x =axisX, y=axisY, kind = 'line',ax=ax1)\n",
    "    \n",
    "    #Instead of trimming the data sets, identify the nearest indices in both dataframes\n",
    "    #sAssumes increase in the q of the X Axis\n",
    "    index_max = ccd0working[axisX].idxmax() #I think this value is trivial...\n",
    "    q_max = ccd0working[axisX].max()\n",
    "    index_min = ccd100working[axisX].idxmin() #I think this value is trivial...\n",
    "    q_min = ccd100working[axisX].min()\n",
    "    \n",
    "    if ((ccd0[axisX].max() > ccd100[axisX].max())or(ccd0[axisX].min() > ccd100[axisX].min())):\n",
    "        raise Exception(\"You might have swapped ccd0 and ccd100 or one set is contained inside the other one\")\n",
    "        \n",
    "    if ((window > 1) or (window < 0)):\n",
    "        raise Exception(\"Window must be a value from 0 to 1 inclusive\")\n",
    "    \n",
    "    #Calculate q if None is provided\n",
    "    if ((q is None) and (regime == \"loglog\")):\n",
    "        q = (q_max-q_min)/(np.log(q_max)-np.log(q_min))\n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: Automatic stitchpoint is at q = \" + str(q))\n",
    "    elif ((q is None) and (regime == \"linear\")):\n",
    "        q = 0.5*(q_max+q_min) #Average based on data points\n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: Automatic stitchpoint is at q = \" + str(q))\n",
    "    else: \n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: Manual stitchpoint is at q = \" + str(q))\n",
    "        \n",
    "    #Error Checking for q value:\n",
    "    if ((q > q_max) or (q < q_min)):\n",
    "        raise Exception(\"Chosen stitchpoint is out of bounds\")\n",
    "        \n",
    "    #Same parameters but of the stitch point\n",
    "    index_stitch_ccd0 = closest(ccd0working,axisX,q)\n",
    "    index_stitch_ccd100 = closest(ccd100working,axisX,q)\n",
    "    \n",
    "    #Identify the Edged for the averaging based on the stitch point \n",
    "    \n",
    "    if (debug == 1):\n",
    "        #Debug Printing Statements\n",
    "        print(\"index_max is \" + str(index_max))\n",
    "        print(\"q_max is \" + str(q_max))\n",
    "        print(\"index_min is \" + str(index_min))\n",
    "        print(\"q_min is \" + str(q_min))\n",
    "        print(\"Stitchpoint index in ccd0 is \" + str(index_stitch_ccd0))\n",
    "        print(\"Stitchpoint index in ccd100 is \" + str(index_stitch_ccd100))\n",
    "    \n",
    "    #Error Check: \n",
    "    if (index_stitch_ccd0 - smoothing < 0):\n",
    "        raise Exception(\"Smoothing window is outside data range\")\n",
    "    if (index_stitch_ccd100 - smoothing > len(ccd100)):\n",
    "        raise Exception(\"Smoothing window is outside data range\")\n",
    "    \n",
    "    if regime == \"linlin\":                                                                                                                                    #Arbitrary +1's are to account for matrix reading skipping the value at the last index\n",
    "        #Identify Slope/intercept of the SAXS data at the stitch point\n",
    "        x0 = np.array(ccd0working.iloc[(index_stitch_ccd0-(smoothing)):(index_stitch_ccd0+1)][axisX]).reshape(-1, 1)\n",
    "        y0 = np.array(ccd0working.iloc[(index_stitch_ccd0-(smoothing)):(index_stitch_ccd0+1)][axisY]).reshape(-1, 1)\n",
    "        lm0 = LinearRegression()\n",
    "        lm0.fit(x0, y0)\n",
    "        intercept_ccd0 = lm0.intercept_[0]\n",
    "        coef_ccd0 = float(lm0.coef_[0])\n",
    "        \n",
    "        if (debug == 1): #Debugging Mode Code\n",
    "            print(\"x0 is \" + str(x0))\n",
    "            print(\"y0 is \" + str(y0))\n",
    "            print(\"Intercept of CCD0 is \"+str(intercept_ccd0))\n",
    "            print(\"Slope of CCD0 is \"+str(coef_ccd0))\n",
    "\n",
    "        if (progress == 1):    \n",
    "            print(\"COMPLETED: CCD0 Linear Regression\")\n",
    "\n",
    "        #Do the Same for the CCD100 data\n",
    "        x100 = np.array(ccd100working.iloc[index_stitch_ccd100:(index_stitch_ccd100+(smoothing+1))][axisX]).reshape(-1, 1)\n",
    "        y100 = np.array(ccd100working.iloc[index_stitch_ccd100:(index_stitch_ccd100+(smoothing+1))][axisY]).reshape(-1, 1)\n",
    "        lm100 = LinearRegression()\n",
    "        lm100.fit(x100, y100)\n",
    "        intercept_ccd100 = lm100.intercept_[0]\n",
    "        coef_ccd100 = float(lm100.coef_[0])\n",
    "        if (debug == 1): #Debugging Mode Code\n",
    "            print(\"x100 is \" + str(x100))\n",
    "            print(\"y100 is \" + str(y100))\n",
    "            print(\"Intercept of CCD100 is \"+str(intercept_ccd100))\n",
    "            print(\"Slope of CCD100 is \"+str(coef_ccd100))\n",
    "            print()\n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: CCD100 Linear Regression\")\n",
    "\n",
    "        #Calculate the Slope and Intercept then Scale the WAXS Data (Linear Realm)\n",
    "        if method == \"value\":\n",
    "            alpha = ccd0working.iloc[index_stitch_ccd0][axisY]/ccd100working.iloc[index_stitch_ccd100][axisY]\n",
    "            beta = \"NA\"\n",
    "            ccd100working[axisY] = ccd100working.apply(lambda x: correction_valuematch(x[axisX],x[axisY],alpha,direction=1),'columns')\n",
    "        elif method == \"slope\":\n",
    "            alpha = coef_ccd0/coef_ccd100\n",
    "            beta = correction_slopematch(q,ccd100working.iloc[index_stitch_ccd100][axisY],alpha,0) - ccd0working.iloc[index_stitch_ccd0][axisY]\n",
    "            ccd100working[axisY] = ccd100working.apply(lambda x: correction_slopematch(x[axisX],x[axisY],alpha,beta,direction=1),'columns')\n",
    "        else:\n",
    "            raise Exception(\"Provided Method may be in error or is not found in this function\")\n",
    "    \n",
    "    #Alternate version where it does the fitting as designed for a log log plot\n",
    "    elif regime == \"loglog\": #NOTE: Done using natural logs instead of base10 logs\n",
    "         #Identify Slope/intercept of the SAXS data at the stitch point\n",
    "        x0 = np.array(ccd0working.iloc[(index_stitch_ccd0-(smoothing)):(index_stitch_ccd0+1)][axisX]).reshape(-1, 1)\n",
    "        y0 = np.array(ccd0working.iloc[(index_stitch_ccd0-(smoothing)):(index_stitch_ccd0+1)][axisY]).reshape(-1, 1)\n",
    "        lm0 = LinearRegression()\n",
    "        lm0.fit(np.log10(x0), np.log10(y0))\n",
    "        intercept_ccd0 = lm0.intercept_[0]\n",
    "        coef_ccd0 = float(lm0.coef_[0])\n",
    "        if (debug == 1): #Debugging Mode Code\n",
    "            print(\"x0 is \" + str(x0))\n",
    "            print(\"y0 is \" + str(y0))\n",
    "            print(\"Intercept of CCD0 is \"+str(intercept_ccd0))\n",
    "            print(\"Slope of CCD0 is \"+str(coef_ccd0))\n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: CCD0 Linear Regression\")\n",
    "\n",
    "        #Do the Same for the CCD100 data\n",
    "        x100 = np.array(ccd100working.iloc[index_stitch_ccd100:(index_stitch_ccd100+(smoothing+1))][axisX]).reshape(-1, 1)\n",
    "        y100 = np.array(ccd100working.iloc[index_stitch_ccd100:(index_stitch_ccd100+(smoothing+1))][axisY]).reshape(-1, 1)\n",
    "        lm100 = LinearRegression()\n",
    "        lm100.fit(np.log10(x100), np.log10(y100))\n",
    "        intercept_ccd100 = lm100.intercept_[0]\n",
    "        coef_ccd100 = float(lm100.coef_[0])\n",
    "        if (debug == 1): #Debugging Mode Code\n",
    "            print(\"x100 is \" + str(x100))\n",
    "            print(\"y100 is \" + str(y100))\n",
    "            print(\"Intercept of CCD100 is \"+str(intercept_ccd100))\n",
    "            print(\"Slope of CCD100 is \"+str(coef_ccd100))\n",
    "            print()\n",
    "        if (progress == 1):\n",
    "            print(\"COMPLETED: CCD100 Linear Regression\")\n",
    "\n",
    "        #Calculate the Slope and Intercept then Scale the WAXS Data (Linear Realm)\n",
    "        if method == \"value\":\n",
    "            alpha = ccd0working.iloc[index_stitch_ccd0][axisY]/ccd100working.iloc[index_stitch_ccd100][axisY]\n",
    "            beta = \"NA\"\n",
    "            ccd100working[axisY] = ccd100working.apply(lambda x: correction_valuematch(x[axisX],x[axisY],alpha,direction=1),'columns')\n",
    "        elif method == \"slope\":\n",
    "            alpha = coef_ccd0/coef_ccd100\n",
    "            beta =  np.log10(ccd0working.iloc[index_stitch_ccd0][axisY]) - np.log10(correction_slopematch_log(q,ccd100working.iloc[index_stitch_ccd100][axisY],alpha))\n",
    "            ccd100working[axisY] = ccd100working.apply(lambda x: correction_slopematch_log(x[axisX],x[axisY],alpha,beta,direction=1),'columns')                                        \n",
    "        else:\n",
    "            raise Exception(\"Provided Method may be in error or is not found in this function\")\n",
    "    else:\n",
    "        raise Exception(\"Provided Regime may be in error or is not found in this function\")\n",
    "            \n",
    "    if (progress == 1):\n",
    "        print(\"COMPLETED: WAXS Data Scaling\")\n",
    "    \n",
    "    if (debug == 1): #Debugging Mode Code\n",
    "        print(\"Alpha is \"+str(alpha)+\"; Beta is \" + str(beta))\n",
    "        if (checkpoint == 1): #Debugging Mode Code\n",
    "            print(\"DATAFRAME CHECKPOINT: 1\")\n",
    "            print(ccd0working)\n",
    "            print(ccd100working)\n",
    "        print()\n",
    "    \n",
    "    if (debug == 1):#Debugging Mode Code\n",
    "        #DEBUG CODE: DELETE LATER - Plots original ccd100 and the new scaled ccd100\n",
    "        ax2 = ccd100.plot(x =axisX, y=axisY, kind = 'line')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_xscale('log')\n",
    "        ccd100working.plot(x =axisX, y=axisY, kind = 'line',ax=ax2)\n",
    "\n",
    "        #DEBUG CODE: DELETE LATER - Plots the new ccd100 and the new ccd0 data before stitching\n",
    "        ax3 = ccd0working.plot(x =axisX, y=axisY, kind = 'line')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.set_xscale('log')\n",
    "        ccd100working.plot(x =axisX, y=axisY, kind = 'line',ax=ax3)\n",
    "    \n",
    "    #Insert new data points to CCD0 in overlap region via linear interpolation based on the overlapping q from WAXS\n",
    "    #All work in this section done on the working CCD0 dataframe\n",
    "    overlap_max = closest_ordered(ccd100working,axisX,q_max) #Establish the index in CCD100 that corresponds to the\n",
    "    #print(\"overlap_max is \" + str(overlap_max))\n",
    "    for index100 in range(0,overlap_max+1):\n",
    "        temp_q = ccd100working.iloc[index100][axisX]\n",
    "        index0 = closest_ordered(ccd0working,axisX,temp_q)\n",
    "        if (temp_q == ccd0working.iloc[index0][axisX]):\n",
    "            continue\n",
    "        newvalue = interpol_lin(ccd0working.iloc[index0][axisX],ccd0working.iloc[index0][axisY],ccd0working.iloc[index0+1][axisX],ccd0working.iloc[index0+1][axisY],temp_q)\n",
    "        ccd0working = pd.DataFrame(np.insert(ccd0working.values, (index0+1), [temp_q,newvalue], axis=0),columns = ccd0working.columns)\n",
    "        if (checkpoint == 1): #Debugging Mode Code\n",
    "            print(str(temp_q)+ \" was most closely found at index \"+ str(index0))\n",
    "    if (progress == 1):\n",
    "        print(\"COMPLETED: CCD0 Linear Interpolation\")\n",
    "    \n",
    "    if (checkpoint == 1): #Debugging Mode Code\n",
    "        print(\"DATAFRAME CHECKPOINT: 2\")\n",
    "        print(ccd0working)\n",
    "        print(ccd100working)\n",
    "        print()\n",
    "    \n",
    "    #Update Important INDEXES now that the blank values have been added\n",
    "    index_max = ccd0working[axisX].idxmax() #I think this value is trivial...\n",
    "    index_min = ccd100working[axisX].idxmin() #I think this value is trivial...\n",
    "    index_stitch_ccd0 = closest_ordered(ccd0,axisX,q)\n",
    "    index_stitch_ccd100 = closest_ordered(ccd100,axisX,q)\n",
    "    \n",
    "    if (debug == 1): #Debugging Mode Code\n",
    "        print(\"Stitchpoint index in ccd0 is \" + str(index_stitch_ccd0))\n",
    "        print(\"Stitchpoint index in ccd100 is \" + str(index_stitch_ccd100))\n",
    "        \n",
    "    #Add blank spots into the ccd100 data\n",
    "    overlap_min = closest_ordered(ccd0,axisX,q_min) #Establish the index in CCD0 that corresponds to the minimum of the overlap\n",
    "    \n",
    "    if (debug == 1): #Debugging Mode Code\n",
    "        print(\"overlap_min is \" + str(overlap_min))\n",
    "    for index0 in range(overlap_min+1,index_max+1):  \n",
    "        temp_q = ccd0working.iloc[index0][axisX]\n",
    "        index100 = closest_ordered(ccd100working,axisX,temp_q)\n",
    "        if (temp_q == ccd100working.iloc[index100][axisX]):\n",
    "            continue\n",
    "        newvalue = interpol_lin(ccd100working.iloc[index100][axisX],ccd100working.iloc[index100][axisY],ccd100working.iloc[index100+1][axisX],ccd100working.iloc[index100+1][axisY],temp_q)\n",
    "        ccd100working = pd.DataFrame(np.insert(ccd100working.values, (index100+1), [temp_q,newvalue], axis=0),columns = ccd100working.columns)\n",
    "        if (checkpoint == 1): #Debugging Mode Code\n",
    "            print(str(index100) + \" out of length: \" + str(len(ccd100working)))\n",
    "            print(str(temp_q)+ \" was most closely found at index \"+ str(index0))\n",
    "    if (progress == 1):\n",
    "        print(\"COMPLETED: CCD100 Linear Interpolation\")\n",
    "    \n",
    "    #Update the values again as I've added numbers again\n",
    "    index_max = ccd0working[axisX].idxmax() #I think this value is trivial...\n",
    "    q_max = ccd0working[axisX].max()\n",
    "    index_min = ccd100working[axisX].idxmin() #I think this value is trivial...\n",
    "    q_min = ccd100working[axisX].min()\n",
    "    index_stitch_ccd0 = closest_ordered(ccd0working,axisX,q)\n",
    "    index_stitch_ccd100 = closest_ordered(ccd100working,axisX,q)\n",
    "    \n",
    "    #Determine the maximal edges of the blended data range in the CCD0 dataframe\n",
    "    distance = (min(q-q_min,q_max-q))*window\n",
    "    window_minQ = q-distance\n",
    "    window_maxQ = q+distance\n",
    "    window_minI = closest_ordered(ccd0working,axisX,window_minQ)\n",
    "    window_maxI = closest_ordered(ccd0working,axisX,window_maxQ)\n",
    "\n",
    "    if (debug == 1): #Debugging Mode Code\n",
    "        if (checkpoint == 1): #Debugging Mode Code\n",
    "            print(\"DATAFRAME CHECKPOINT: 3\")\n",
    "            print(ccd0working)\n",
    "            print(ccd100working)\n",
    "        print(\"window_minQ is \"+str(window_minQ))\n",
    "        print(\"window_maxQ is \"+str(window_maxQ))\n",
    "        print()\n",
    "\n",
    "    #Weighted averaging based on the Windowed parameter\n",
    "    #NOTE This line of code only works if the previous one has successfully \n",
    "    #added all the correct q's\n",
    "    for index0 in range(window_minI,(window_maxI+1)):\n",
    "        index100 = closest_ordered(ccd100working,axisX,ccd0working.iloc[index0][axisX]) #Error is here I think\n",
    "        weight = fraction_symm(ccd0working.iloc[index0][axisX],window_minQ,window_maxQ)\n",
    "        ccd0working.iloc[index0][axisY] = ((1-weight)*(ccd0working.iloc[index0][axisY]))+((weight)*(ccd100working.iloc[index100][axisY]))\n",
    "        if (checkpoint == 1): #Debugging Mode Code\n",
    "            print(\"Current Index0: \"+str(index0)+\"; Current Index100: \"+str(index100))\n",
    "            print(\"ccd0 is \"+str(ccd0working.iloc[index0][axisX])+\"; ccd100 is \"+str(ccd100working.iloc[index100][axisX])+\";\")\n",
    "            print(\"Weight is \" + str(weight) +\"; Blended is \" +str(ccd0working.iloc[index0][axisY]))\n",
    "            print()\n",
    "\n",
    "    if (progress == 1):\n",
    "        print(\"COMPLETED: Weighted Data Blending\")\n",
    "    \n",
    "    ccd0working = ccd0working[:window_maxI+1] #Trim off excess CCD0 data above the window\n",
    "    #Will be added later in concatenation\n",
    "    \n",
    "    if (checkpoint == 1): #Debugging Mode Code\n",
    "        print(\"DATAFRAME CHECKPOINT: 4\")\n",
    "        print(ccd0working)\n",
    "        print(ccd100working)\n",
    "        print()\n",
    "    \n",
    "    #Concatenate the remaining working ccd100 data to the now blended ccd0 data\n",
    "    overlap_max_index100 = closest_ordered(ccd100working,axisX,window_maxQ)\n",
    "    \n",
    "    final = pd.concat([ccd0working, ccd100working[overlap_max_index100+1:]])\n",
    "    final.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    if (checkpoint == 1): #Debugging Mode Code\n",
    "        print(\"DATAFRAME CHECKPOINT: FINAL\")\n",
    "        print(final)\n",
    "        print()\n",
    "    \n",
    "    return  final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9e391",
   "metadata": {},
   "source": [
    "### Analysis: sample_stitch two lists of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c61b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desc: This function ingests two datasets and their lists of energies and then combines them using the analysis_stitch \n",
    "#function. Accepts 2 lists of dataframes and their associated lists of energies\n",
    "def sample_stitch(axisX,axisY,ccd0_data,ccd0_energy,ccd100_data,ccd100_energy,q=None,smoothing=1,window=1,regime=\"loglog\",method=\"slope\"):\n",
    "    #Create copies\n",
    "    #print(\"ccd0_data length: \"+str(len(ccd0_data)))\n",
    "    #print(\"ccd0_energies length: \"+str(len(ccd0_energy)))\n",
    "    #print(\"ccd100_data length: \"+str(len(ccd100_data)))\n",
    "    #print(\"ccd100_energies length: \"+str(len(ccd100_energy)))\n",
    "\n",
    "    \n",
    "    #Size Checks for CCD0 and CCD100\n",
    "    if (len(ccd0_data) != len(ccd0_energy)):\n",
    "        raise Exception(\"Mismatching energies and data for ccd0\")\n",
    "    if (len(ccd100_data) != len(ccd100_energy)):\n",
    "        raise Exception(\"Mismatching energies and data for ccd100\")\n",
    "    \n",
    "    #Create common energies list and trim off the extra energies from the imported datasets\n",
    "    energies = intersection(ccd0_energy,ccd100_energy)\n",
    "    \n",
    "    #Trim the ccd0 dataset\n",
    "    for index in range(len(ccd0_energy)):\n",
    "        if not (ccd0_energy[index] in energies):\n",
    "            del ccd0_data[index]\n",
    "    \n",
    "    #Trim out ccd100 dataframes\n",
    "    for index in range(len(ccd100_energy)):\n",
    "        if not (ccd100_energy[index] in energies):\n",
    "            del ccd100_data[index]\n",
    "    \n",
    "    #Create blank list for stitching dataframes INTO\n",
    "    stitched = []\n",
    "\n",
    "    #Progress Bar:\n",
    "    #bar1 = tqdm(total=len(energies), position=0, dynamic_ncols=True, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]')\n",
    "    #Loop to Access Each Dataframe in the list\n",
    "    for index in tqdm(range(len(energies)),desc=\"Sample In Progress\",colour='blue'):\n",
    "    #for index in tqdm(range(len(energies)), position=0, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]'):\n",
    "        temp_df = analysis_stitch(axisX,axisY,ccd0_data[index],ccd100_data[index],q,smoothing,window,regime)\n",
    "        stitched.append(temp_df)\n",
    "        #bar1.update(int(1))\n",
    "        #print(str(index+1)+\"/\"+str(len(energies))+\" dataframes complete\")\n",
    "        #print()\n",
    "        #Stich the dataframes into the new list\n",
    "        \n",
    "    #Return the new stuff as tuples\n",
    "    return (energies, stitched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b148abb",
   "metadata": {},
   "source": [
    "### analysis_insertpoints: alter original dataframes to have the same x points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0a17840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function accepts 2 dataframes and ALTERS THEIR ORIGINALS to have the same x points using linear interpolation\n",
    "# Function has a future option function to add the lagging ends on either side \n",
    "\n",
    "\n",
    "def analysis_insertpoints(df1,df2,axisX,axisY,edges=False):\n",
    "\n",
    "    #dataframe parameters:\n",
    "    max_1 = df1[axisX].max()\n",
    "    max_2 = df2[axisX].max()\n",
    "    min_1 = df1[axisX].min()\n",
    "    min_2 = df2[axisX].min()\n",
    "    overlap_max = min(max_1,max_2)\n",
    "    overlap_min = max(min_1,min_2)\n",
    "    overall_max = max(max_1,max_2)\n",
    "    overall_min = min(min_1,min_2)\n",
    "    df1_idxmin = closest_ordered(df1,axisX,overlap_min)\n",
    "    df1_idxmax = closest_ordered(df1,axisX,overlap_max)\n",
    "    df2_idxmin = closest_ordered(df2,axisX,overlap_min)\n",
    "    df2_idxmax = closest_ordered(df2,axisX,overlap_max)\n",
    "\n",
    "    #overlap fraction check\n",
    "    overlapfrac = ((overlap_max-overlap_min)/(overall_max-overall_min))\n",
    "\n",
    "    #print(overlapfrac)\n",
    "\n",
    "    if (overlapfrac<0):\n",
    "        return Exception(\"There is no overlap between these dataframes, check again\")\n",
    "    if (overlapfrac<0.5):\n",
    "        print(\"Your dataframes only overlap \" + str(overlapfrac*100)+\"%, are you sure?\")\n",
    "    #if ((overlapfrac)and(overlapfrac >=0)):\n",
    "    #    return Exception(\"There is no overlap between these dataframes, check again\")\n",
    "\n",
    "\n",
    "    #Ideally Want to run the code as plastic as possible so the df1 and df2 don't have to be in order\n",
    "\n",
    "    for index1 in range(df1_idxmin+1,df1_idxmax+1):\n",
    "        temp_X = df1.iloc[index1][axisX]\n",
    "        index2 = closest_ordered(df2,axisX,temp_X)\n",
    "        if (temp_X == df2.iloc[index2][axisX]):\n",
    "            continue\n",
    "        newvalue = interpol_lin(df2.iloc[index2][axisX],df2.iloc[index2][axisY],df2.iloc[index2+1][axisX],df2.iloc[index2+1][axisY],temp_X)\n",
    "        df2 = pd.DataFrame(np.insert(df2.values, (index2+1), [temp_X,newvalue], axis=0),columns = df2.columns)\n",
    "    \n",
    "    for index2 in range(df2_idxmin+1,df2_idxmax+1):\n",
    "        temp_X = df2.iloc[index2][axisX]\n",
    "        index1 = closest_ordered(df1,axisX,temp_X)\n",
    "        if (temp_X == df1.iloc[index1][axisX]):\n",
    "            continue\n",
    "        newvalue = interpol_lin(df1.iloc[index1][axisX],df1.iloc[index1][axisY],df1.iloc[index1+1][axisX],df1.iloc[index1+1][axisY],temp_X)\n",
    "        df1 = pd.DataFrame(np.insert(df1.values, (index1+1), [temp_X,newvalue], axis=0),columns = df1.columns)\n",
    "\n",
    "\n",
    "    return (df1,df2,overlapfrac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b6736",
   "metadata": {},
   "source": [
    "### analysis_baseline: \n",
    "Flat addition or subtraction of a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5fb7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IN DEVELOPMENT\n",
    "def analysis_baseline(raw,base,axisX,axisY,flat=0,dir=-1):\n",
    "\n",
    "    f_raw,f_base,f_overlap = analysis_insertpoints(raw,base,axisX,axisY)\n",
    "\n",
    "    base_min = base[axisX].min()\n",
    "    base_max = base[axisX].max()\n",
    "\n",
    "    #print(\"f_base:\")\n",
    "    #print(f_base)\n",
    "\n",
    "    if (f_overlap<0.80):\n",
    "        print(\"WARNING: Baseline only covers \"+str(f_overlap*100)+\"% percent of the raw data\")\n",
    "    if (not((dir==1)or(dir==-1))):\n",
    "        raise Exception(\"Direction of the adjustment can only be up (1) or down (-1)\")\n",
    "\n",
    "    for index in range(len(f_raw)):\n",
    "        tempX = f_raw.iloc[index][axisX]\n",
    "        #print(\"Current Position --- \"+ str(index))\n",
    "        #print(f_base.iloc[index][axisX]<base_min)\n",
    "        if f_raw.iloc[index][axisX] < base_min:\n",
    "            #print(\"Statement 1\")\n",
    "            base_idx = 0\n",
    "        elif f_raw.iloc[index][axisX] > base_max:\n",
    "            #print(\"Statement 2\")\n",
    "            base_idx = len(f_base)-1\n",
    "        else:\n",
    "            #print(\"Statement 3\")\n",
    "            base_idx = closest_ordered(f_base,axisX,tempX)\n",
    "        adjust = (flat + f_base.iloc[base_idx][axisY])*dir\n",
    "        f_raw.iloc[index][axisY] = f_raw.iloc[index][axisY] + adjust\n",
    "\n",
    "    return f_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96191a1d",
   "metadata": {},
   "source": [
    "### analysis_opvsingle: Jsc, Voc, FF, PCE\n",
    "Intake a single JV curve and output a tuple containing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0c1fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FUTURE: Consider whether to absolute value the VOC or to leave it as is. Based on PRECEDENT I should absolute value it\n",
    "#ATM I have this setup to accept a name and channel if provided\n",
    "def analysis_opvsingle(df,area,powerin,name=\"TBD\",channel=\"Z\",trim=1):\n",
    "    #print(df)\n",
    "    df_trim = df.iloc[1:-7].copy()\n",
    "    df_trim = df_trim.reset_index(drop=True)\n",
    "    #print(df_trim)\n",
    "\n",
    "    #Modify the 'J' (current) column to be current density in mA/cm^2\n",
    "    df_trim['J'] = df_trim['J'].apply(lambda x: x*1000/area) #Change units to ones we know, mA/cm^2\n",
    "    #Note: test_sample is a list of dataframe, so test_sample[#] is a dataframe\n",
    "\n",
    "    #Task Locate Voc and Jsc by nearest whole value/index\n",
    "    jsc_idx = closest(df_trim,\"V\",0)\n",
    "    #jsc_idx = closest(df,\"V\",0)\n",
    "    jsc = df_trim[\"J\"][jsc_idx]\n",
    "    #print(jsc_idx)\n",
    "    #print(\"Jsc = \" + str(jsc))\n",
    "    voc_idx = closest(df_trim,\"J\",0)\n",
    "    #voc_idx = closest(df,\"J\",0)\n",
    "    voc =  df_trim[\"V\"][voc_idx]\n",
    "    #print(voc_idx)\n",
    "    #print(\"Voc = \" + str(voc))\n",
    "    p_theory = jsc*voc #units of mW/cm^2\n",
    "    #print(\"Ptheory = \" + str(p_theory))\n",
    "\n",
    "    #Create a new column with power shown as a function of voltage/current\n",
    "    df_trim['P'] = df_trim['V'] * df_trim['J']\n",
    "    #print(\"\")\n",
    "    #print(\"New Dataframe\")\n",
    "    #print(df)\n",
    "    #df1 = df.iloc[jsc_idx:voc_idx]\n",
    "    #print(df1)\n",
    "\n",
    "    #Boolean check for order of Jsc and voc idx --> Needed for zoomed check of max power point\n",
    "    #Note; this function needs to be able to en masse go through data. So I will eventually need a way to avoid errors \n",
    "    if (jsc_idx <= voc_idx):\n",
    "        idx_A = jsc_idx\n",
    "        idx_B = voc_idx\n",
    "    if (jsc_idx > voc_idx):\n",
    "        idx_B = jsc_idx\n",
    "        idx_A = voc_idx\n",
    "\n",
    "\n",
    "    #print(df['P'].iloc[jsc_idx:voc_idx].idxmin())\n",
    "    #pmax_idx = df['P'].iloc[jsc_idx:voc_idx].idxmin() #task locate the index of the max power point\n",
    "    pmax_idx = df_trim['P'].iloc[idx_A:idx_B].idxmin() #task locate the index of the max power point\n",
    "\n",
    "    p_max = df_trim['P'].iloc[pmax_idx]\n",
    "    #print(p_max)\n",
    "\n",
    "    pce = -100*(p_max/(powerin*1000)) #calculate PCE\n",
    "    #print(\"PCE = \" + str(pce)+\"%\")\n",
    "    ff = (p_max/p_theory)\n",
    "    #print(\"FF = \" + str(ff))\n",
    "\n",
    "    #task Export all values as a dictionary\n",
    "    f_opv = {\n",
    "        \"name\": name,\n",
    "        \"channel\": channel,\n",
    "        \"pce\": round(pce,3),\n",
    "        \"jsc\": round(jsc,3),\n",
    "        \"voc\": round(abs(voc),3),\n",
    "        \"ff\": round(ff,3)\n",
    "    }\n",
    "\n",
    "    return f_opv\n",
    "    \n",
    "#Still must correct for flipped order Jsc, VOC --> Basic boolean?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a96c75",
   "metadata": {},
   "source": [
    "### analysis_opvset: Takes in folder structure with samples, calculates performance parameters and outputs as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3676dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_opvset(folder, area, pin,vfirst = 1,export=1,exportpath=\"\",plot=0,f_trim=1,max_channels=6):\n",
    "    #output = pd.DataFrame()\n",
    "    output = pd.DataFrame(columns=[\"name\",\"channel\",'pce','jsc','voc','ff'])\n",
    "\n",
    "    #Check just in case receiver data reverses the current and voltage\n",
    "    if vfirst == 1:\n",
    "        col1 = \"V\"\n",
    "        col2 = 'J'\n",
    "    else:\n",
    "        col1 = 'J'\n",
    "        col2 = 'V'\n",
    "    \n",
    "    #Error Checking\n",
    "    #if not (folder[-1] == \"/\"):\n",
    "    #    raise Exception(\"Remember to include a trailing slash on the folder directory\")\n",
    "    process_directory(folder) #Check to make sure everything has .dat on it\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.dat') and (not (filename.startswith('.') or filename.startswith('dark'))):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                folder_name = os.path.basename(dirpath)\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                print(str(folder_name)+\"/\"+str(filename)+\" --> Reading\")\n",
    "                df1 = pd.read_csv(file_path, comment='#',sep='\\t',usecols = [0,1], names = [col1,col2],skiprows=1)\n",
    "                #temp_cell = analysis_opvsingle(df1,area,pin,folder_name,channel=string.ascii_lowercase[((int(name[-1])-1)%max_channels)+1],trim=f_trim)\n",
    "                #print(name.split('_')[-1])\n",
    "                print(((int(name.split('_')[-1])-1) % max_channels))\n",
    "                print(string.ascii_lowercase[((int(name.split('_')[-1])-1) % max_channels)])\n",
    "                temp_cell = analysis_opvsingle(df1,area,pin,folder_name,channel=string.ascii_lowercase[((int(name.split('_')[-1])-1) % max_channels)],trim=f_trim)\n",
    "                print(temp_cell)\n",
    "                #ChatGPT convert_filename = lambda name, max_channels=6: string.ascii_uppercase[(int(name.split('_')[-1]) - 1) % max_channels] \n",
    "                #df2 = pd.DataFrame(temp_cell)\n",
    "                #df2 = pd.DataFrame(analysis_opvsingle(df1,area,pin,folder_name,channel=string.ascii_uppercase[int(name[-1])-1]))\n",
    "                #output = pd.concat([output, df2], ignore_index=True)\n",
    "                output = pd.concat([output,pd.DataFrame([temp_cell])],ignore_index=True)\n",
    "                # df = pd.concat([df, pd.DataFrame([data_dict])], ignore_index=True) #sample code\n",
    "                #print(df2)\n",
    "                #process_dat_file(file_path)\n",
    "                #print(f\"File: {filename}, Folder: {folder_name}\")\n",
    "                print(str(folder_name)+\"/\"+str(filename)+\" --> Completed\")\n",
    "\n",
    "    output_sorted = output.sort_values(by=['name','channel'])\n",
    "    output_sorted = output_sorted.reset_index(drop=True)\n",
    "    print(\"**OPV ANALYSIS COMPLETE**\")     \n",
    "\n",
    "\n",
    "    #Error Checking\n",
    "    if len(output)==0:\n",
    "        raise Exception(\"You didn't extract any data, check to see if you have the correct folder directory\")\n",
    "\n",
    "    #print(output.head())\n",
    "    #print(output.head()\n",
    "\n",
    "    if export == 1:\n",
    "        path=\"Data_Export/\"+exportpath\n",
    "        filepath = Path(path)\n",
    "        filepath.mkdir(parents=True, exist_ok=True)\n",
    "        output_sorted.to_csv(path/Path(\"opvstats.csv\"), index = False)\n",
    "\n",
    "\n",
    "    return output_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3babd",
   "metadata": {},
   "source": [
    "## Combination Functions: Upper Hierarchy handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5343c",
   "metadata": {},
   "source": [
    "### session_stitch: Processing of an entire set of data and energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_stitch(list_data,list_energies,selectenergies,singleenergy,f_smooth=10,f_window=1,f_method='slope',f_regime='loglog',xaxis='q',yaxis='Intensity',f_suffix=\"\"):\n",
    "    print(\"WARNING: Function is meant as an OVERVIEW summary data processing technique, so most functions called within use default values. If you wish to customize fit parameters either update this function,\\\n",
    "    or simply copy the loop structure contained within and customize it in working code\")\n",
    "    print(\"Code has also not been optimized to account for differing sorting order when importing USE AT OWN RISK\")\n",
    "\n",
    "    #Size Checks for data and energies\n",
    "    if (len(list_data) < len(list_energies)):\n",
    "        raise Exception(\"You have more energies lists than data lists\")\n",
    "    if (len(list_data) > len(list_energies)):\n",
    "        raise Exception(\"Mismatching energies and data for ccd100\")\n",
    "    \n",
    "\n",
    "    length = len(list_data)\n",
    "    #print(length)\n",
    "    if length % 2 != 0:\n",
    "        raise Exception(\"Uneven number of dataframes supplied, check for missing CCD0 or CCD100\")\n",
    "\n",
    "    for i in tqdm(range(int(length/2)),desc=\"Session Progress\",colour='red'):\n",
    "        t_energies,t_data = sample_stitch(xaxis,yaxis,globals()[list_data[2*i+1]],globals()[list_energies[2*i+1]],globals()[list_data[2*i]],globals()[list_energies[2*i]],\\\n",
    "            smoothing=f_smooth,window=f_window,method=f_method,regime=f_regime)\n",
    "        t_name = list_data[2*i][:-7] +\"_\"+f_suffix\n",
    "        t_path = t_name + \"/\"\n",
    "        #print(t_name) #Debug Code\n",
    "        export_csv(t_name,t_data,t_energies)\n",
    "        plot_allenergies(t_name,t_data,t_energies,path=t_path)\n",
    "        plot_selectenergy(t_name,t_data,t_energies,selectenergies,path=t_path)\n",
    "        plot_singleenergy(t_name,t_data,t_energies, singleenergy,path=t_path)\n",
    "        \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ecb0cc",
   "metadata": {},
   "source": [
    "### session_opv: Processing of an entire day of opv data from hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#TBD code for session importing and exporting\n",
    "test = \"Test\"\n",
    "\n",
    "print(7%7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77624f8",
   "metadata": {},
   "source": [
    "# Troubleshooting and Clipboard Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0723ab",
   "metadata": {},
   "source": [
    "## Template Analysis Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485165e",
   "metadata": {},
   "source": [
    "#### OPV Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0b67ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/light_2.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '3', 'channel': 'b', 'pce': 10.276, 'jsc': 21.568, 'voc': 0.77, 'ff': 0.598}\n",
      "3/light_2.dat --> Completed\n",
      "3/light_22.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '3', 'channel': 'd', 'pce': 9.863, 'jsc': 21.304, 'voc': 0.79, 'ff': 0.567}\n",
      "3/light_22.dat --> Completed\n",
      "3/light_23.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '3', 'channel': 'e', 'pce': 10.445, 'jsc': 20.881, 'voc': 0.78, 'ff': 0.62}\n",
      "3/light_23.dat --> Completed\n",
      "3/light_3.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '3', 'channel': 'c', 'pce': 11.025, 'jsc': 21.417, 'voc': 0.77, 'ff': 0.647}\n",
      "3/light_3.dat --> Completed\n",
      "3/light_1.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '3', 'channel': 'a', 'pce': 8.265, 'jsc': 20.112, 'voc': 0.76, 'ff': 0.523}\n",
      "3/light_1.dat --> Completed\n",
      "3/light_21.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '3', 'channel': 'c', 'pce': 11.44, 'jsc': 21.06, 'voc': 0.78, 'ff': 0.674}\n",
      "3/light_21.dat --> Completed\n",
      "3/light_20.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '3', 'channel': 'b', 'pce': 11.026, 'jsc': 21.392, 'voc': 0.79, 'ff': 0.631}\n",
      "3/light_20.dat --> Completed\n",
      "3/light_4.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '3', 'channel': 'd', 'pce': 2.074, 'jsc': 4.284, 'voc': 0.69, 'ff': 0.679}\n",
      "3/light_4.dat --> Completed\n",
      "3/light_18.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '3', 'channel': 'f', 'pce': 9.444, 'jsc': 20.136, 'voc': 0.72, 'ff': 0.63}\n",
      "3/light_18.dat --> Completed\n",
      "3/light_24.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '3', 'channel': 'f', 'pce': 10.943, 'jsc': 20.872, 'voc': 0.78, 'ff': 0.65}\n",
      "3/light_24.dat --> Completed\n",
      "3/light_19.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '3', 'channel': 'a', 'pce': 9.548, 'jsc': 21.178, 'voc': 0.79, 'ff': 0.552}\n",
      "3/light_19.dat --> Completed\n",
      "3/light_5.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '3', 'channel': 'e', 'pce': 0.613, 'jsc': 1.46, 'voc': 0.76, 'ff': 0.534}\n",
      "3/light_5.dat --> Completed\n",
      "3/light_7.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '3', 'channel': 'a', 'pce': 10.836, 'jsc': 24.477, 'voc': 0.8, 'ff': 0.535}\n",
      "3/light_7.dat --> Completed\n",
      "3/light_6.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '3', 'channel': 'f', 'pce': 0.0, 'jsc': 4.538, 'voc': 0.03, 'ff': 0.001}\n",
      "3/light_6.dat --> Completed\n",
      "3/light_17.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '3', 'channel': 'e', 'pce': 9.342, 'jsc': 20.681, 'voc': 0.75, 'ff': 0.583}\n",
      "3/light_17.dat --> Completed\n",
      "3/light_16.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '3', 'channel': 'd', 'pce': 8.713, 'jsc': 21.371, 'voc': 0.74, 'ff': 0.533}\n",
      "3/light_16.dat --> Completed\n",
      "3/light_8.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '3', 'channel': 'b', 'pce': 10.853, 'jsc': 25.091, 'voc': 0.79, 'ff': 0.53}\n",
      "3/light_8.dat --> Completed\n",
      "3/light_14.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '3', 'channel': 'b', 'pce': 9.497, 'jsc': 21.502, 'voc': 0.74, 'ff': 0.577}\n",
      "3/light_14.dat --> Completed\n",
      "3/light_15.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '3', 'channel': 'c', 'pce': 0.815, 'jsc': 1.95, 'voc': 0.72, 'ff': 0.562}\n",
      "3/light_15.dat --> Completed\n",
      "3/light_9.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '3', 'channel': 'c', 'pce': 12.918, 'jsc': 25.161, 'voc': 0.79, 'ff': 0.629}\n",
      "3/light_9.dat --> Completed\n",
      "3/light_11.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '3', 'channel': 'e', 'pce': 10.246, 'jsc': 19.973, 'voc': 0.79, 'ff': 0.628}\n",
      "3/light_11.dat --> Completed\n",
      "3/light_10.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '3', 'channel': 'd', 'pce': 9.432, 'jsc': 19.447, 'voc': 0.79, 'ff': 0.594}\n",
      "3/light_10.dat --> Completed\n",
      "3/light_12.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '3', 'channel': 'f', 'pce': 5.748, 'jsc': 19.37, 'voc': 0.71, 'ff': 0.404}\n",
      "3/light_12.dat --> Completed\n",
      "3/light_13.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '3', 'channel': 'a', 'pce': 8.552, 'jsc': 21.104, 'voc': 0.74, 'ff': 0.53}\n",
      "3/light_13.dat --> Completed\n",
      "2/light_2.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '2', 'channel': 'b', 'pce': 11.43, 'jsc': 22.649, 'voc': 0.8, 'ff': 0.61}\n",
      "2/light_2.dat --> Completed\n",
      "2/light_22.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '2', 'channel': 'd', 'pce': 1.556, 'jsc': 2.624, 'voc': 0.79, 'ff': 0.726}\n",
      "2/light_22.dat --> Completed\n",
      "2/light_23.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '2', 'channel': 'e', 'pce': 0.745, 'jsc': 1.348, 'voc': 0.79, 'ff': 0.677}\n",
      "2/light_23.dat --> Completed\n",
      "2/light_3.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '2', 'channel': 'c', 'pce': 11.785, 'jsc': 21.603, 'voc': 0.79, 'ff': 0.668}\n",
      "2/light_3.dat --> Completed\n",
      "2/light_1.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '2', 'channel': 'a', 'pce': 8.6, 'jsc': 22.912, 'voc': 0.79, 'ff': 0.46}\n",
      "2/light_1.dat --> Completed\n",
      "2/light_21.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '2', 'channel': 'c', 'pce': 13.299, 'jsc': 25.565, 'voc': 0.8, 'ff': 0.629}\n",
      "2/light_21.dat --> Completed\n",
      "2/light_20.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '2', 'channel': 'b', 'pce': 11.947, 'jsc': 25.249, 'voc': 0.79, 'ff': 0.579}\n",
      "2/light_20.dat --> Completed\n",
      "2/light_4.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '2', 'channel': 'd', 'pce': 4.086, 'jsc': 6.838, 'voc': 0.79, 'ff': 0.732}\n",
      "2/light_4.dat --> Completed\n",
      "2/light_18.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '2', 'channel': 'f', 'pce': 10.7, 'jsc': 20.182, 'voc': 0.79, 'ff': 0.649}\n",
      "2/light_18.dat --> Completed\n",
      "2/light_24.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '2', 'channel': 'f', 'pce': 1.559, 'jsc': 2.589, 'voc': 0.8, 'ff': 0.728}\n",
      "2/light_24.dat --> Completed\n",
      "2/light_19.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '2', 'channel': 'a', 'pce': 11.069, 'jsc': 25.754, 'voc': 0.8, 'ff': 0.52}\n",
      "2/light_19.dat --> Completed\n",
      "2/light_5.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '2', 'channel': 'e', 'pce': 2.07, 'jsc': 3.535, 'voc': 0.78, 'ff': 0.726}\n",
      "2/light_5.dat --> Completed\n",
      "2/light_7.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '2', 'channel': 'a', 'pce': 11.082, 'jsc': 26.841, 'voc': 0.76, 'ff': 0.525}\n",
      "2/light_7.dat --> Completed\n",
      "2/light_6.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '2', 'channel': 'f', 'pce': 11.535, 'jsc': 20.39, 'voc': 0.79, 'ff': 0.693}\n",
      "2/light_6.dat --> Completed\n",
      "2/light_17.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '2', 'channel': 'e', 'pce': 10.567, 'jsc': 20.555, 'voc': 0.79, 'ff': 0.629}\n",
      "2/light_17.dat --> Completed\n",
      "2/light_16.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '2', 'channel': 'd', 'pce': 9.818, 'jsc': 21.139, 'voc': 0.79, 'ff': 0.569}\n",
      "2/light_16.dat --> Completed\n",
      "2/light_8.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '2', 'channel': 'b', 'pce': 11.377, 'jsc': 26.693, 'voc': 0.73, 'ff': 0.565}\n",
      "2/light_8.dat --> Completed\n",
      "2/light_14.dat --> Reading\n",
      "1\n",
      "b\n",
      "{'name': '2', 'channel': 'b', 'pce': 12.723, 'jsc': 25.603, 'voc': 0.79, 'ff': 0.608}\n",
      "2/light_14.dat --> Completed\n",
      "2/light_15.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '2', 'channel': 'c', 'pce': 12.999, 'jsc': 24.265, 'voc': 0.79, 'ff': 0.656}\n",
      "2/light_15.dat --> Completed\n",
      "2/light_9.dat --> Reading\n",
      "2\n",
      "c\n",
      "{'name': '2', 'channel': 'c', 'pce': 12.287, 'jsc': 24.71, 'voc': 0.74, 'ff': 0.65}\n",
      "2/light_9.dat --> Completed\n",
      "2/light_11.dat --> Reading\n",
      "4\n",
      "e\n",
      "{'name': '2', 'channel': 'e', 'pce': 9.264, 'jsc': 20.955, 'voc': 0.73, 'ff': 0.586}\n",
      "2/light_11.dat --> Completed\n",
      "2/light_10.dat --> Reading\n",
      "3\n",
      "d\n",
      "{'name': '2', 'channel': 'd', 'pce': 9.357, 'jsc': 20.731, 'voc': 0.78, 'ff': 0.56}\n",
      "2/light_10.dat --> Completed\n",
      "2/light_12.dat --> Reading\n",
      "5\n",
      "f\n",
      "{'name': '2', 'channel': 'f', 'pce': 9.552, 'jsc': 20.243, 'voc': 0.73, 'ff': 0.625}\n",
      "2/light_12.dat --> Completed\n",
      "2/light_13.dat --> Reading\n",
      "0\n",
      "a\n",
      "{'name': '2', 'channel': 'a', 'pce': 11.542, 'jsc': 26.037, 'voc': 0.8, 'ff': 0.536}\n",
      "2/light_13.dat --> Completed\n",
      "**OPV ANALYSIS COMPLETE**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/j22d_ff10qsd36c484b1r8zw0000gn/T/ipykernel_66215/810520660.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output = pd.concat([output,pd.DataFrame([temp_cell])],ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Temporary test cell for OPV Data Analysis function\n",
    "#test_sample = import_data_dat(\"light_1\",\"20230901/ADoE_04\",\"\",\".dat\",independent=\"V\",dependent=\"J\")\n",
    "#print(\"ADoE_4 (A) has \" + str(len(test_sample)) + \" dataframes in it\")\n",
    "area = 0.162 #cm^2 0.172\n",
    "#p_in = 0.0984 # W/cm^2\n",
    "p_in = 0.09676 # W/cm^2 0.0992\n",
    "#print(analysis_opvsingle(test_sample,area,p_in,\"ADoE_4 (A)\"))\n",
    "#test = pd.read_csv(\"20230901_Debug/ADoE_04/light_1.dat\", comment='#',sep='\\t',usecols = [0,1], names = ['V','J'],skiprows=1)\n",
    "#print(\"OUTSIDE FUNCTION\")\n",
    "#print(test)\n",
    "#test2 = test.iloc[1:-1].copy()\n",
    "#test2 = test2.reset_index(drop=True)\n",
    "#print(test2)\n",
    "#print(\"FUNCTION TEST\")\n",
    "#test_res = analysis_opvsingle(test2,area,p_in,name = \"Debug\")\n",
    "#test_res2 = analysis_opvsingle(test,area,p_in,name = \"Debug\")\n",
    "#print(test_res)\n",
    "#print(test_res2)\n",
    "\n",
    "samples = analysis_opvset(\"20210722\",area,p_in,vfirst=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47eabc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name channel     pce     jsc   voc     ff\n",
      "0      1       a  10.929  21.875  0.81  0.597\n",
      "1      1       a  10.592  22.573  0.81  0.560\n",
      "2      1       a  11.584  25.955  0.81  0.533\n",
      "3      1       a  11.447  23.506  0.81  0.582\n",
      "4      1       b  11.008  20.952  0.79  0.643\n",
      "..   ...     ...     ...     ...   ...    ...\n",
      "607    9       e   8.359  20.866  0.76  0.510\n",
      "608    9       z   8.857  23.325  0.78  0.471\n",
      "609    9       z   9.260  24.618  0.75  0.485\n",
      "610    9       z   8.396  23.204  0.75  0.467\n",
      "611    9       z   8.344  20.781  0.76  0.511\n",
      "\n",
      "[612 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(samples)\n",
    "samples_sorted = samples.sort_values(by=['name'])\n",
    "samples_sorted = samples_sorted.reset_index(drop=True)\n",
    "#print(samples_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cdcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SW_0F_A_CCD100', 'SW_0F_A_CCD0']\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[39m#print(type(Ace_2_A_CCD0))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#print(Ace_2_A_CCD0['q'])\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m t_energies,t_stitched \u001b[39m=\u001b[39m sample_stitch(\u001b[39m'\u001b[39;49m\u001b[39mq\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mIntensity\u001b[39;49m\u001b[39m'\u001b[39;49m,SW_0F_A_CCD0,SW_0F_A_CCD0_energies,SW_0F_A_CCD100,SW_0F_A_CCD100_energies,smoothing\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m \u001b[39m#print(session_data)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#print(session_energies)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#print(pm6y6_high_2_a_CCD0)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBLANKBLANK\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[143], line 36\u001b[0m, in \u001b[0;36msample_stitch\u001b[0;34m(axisX, axisY, ccd0_data, ccd0_energy, ccd100_data, ccd100_energy, q, smoothing, window, regime, method)\u001b[0m\n\u001b[1;32m     31\u001b[0m stitched \u001b[39m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[39m#Progress Bar:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m#bar1 = tqdm(total=len(energies), position=0, dynamic_ncols=True, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m#Loop to Access Each Dataframe in the list\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(energies)),desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSample In Progress\u001b[39;49m\u001b[39m\"\u001b[39;49m,colour\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mblue\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[1;32m     37\u001b[0m \u001b[39m#for index in tqdm(range(len(energies)), position=0, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]'):\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     temp_df \u001b[39m=\u001b[39m analysis_stitch(axisX,axisY,ccd0_data[index],ccd100_data[index],q,smoothing,window,regime)\n\u001b[1;32m     39\u001b[0m     stitched\u001b[39m.\u001b[39mappend(temp_df)\n",
      "File \u001b[0;32m~/Developer/Tools_DataAnalysis/venv/lib/python3.9/site-packages/tqdm/notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    232\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Tools_DataAnalysis/venv/lib/python3.9/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "t_folder = \"./Test_RawData/\"\n",
    "t_prefix = \"8257_\"\n",
    "t_suffix = \"_100_180_10.dat\"\n",
    "x = 'q'\n",
    "y = 'Intensity'\n",
    "\n",
    "#session_data, session_energies = import_set(t_folder,t_prefix,t_suffix,x,y)\n",
    "#print()\n",
    "#print(type(Ace_2_A_CCD0))\n",
    "#print(Ace_2_A_CCD0['q'])\n",
    "#t_energies,t_stitched = sample_stitch('q','Intensity',SW_0F_A_CCD0,SW_0F_A_CCD0_energies,SW_0F_A_CCD100,SW_0F_A_CCD100_energies,smoothing=10)\n",
    "\n",
    "\n",
    "#print(session_data)\n",
    "#print(session_energies)\n",
    "#print(pm6y6_high_2_a_CCD0)\n",
    "print(\"BLANKBLANK\")\n",
    "\n",
    "\n",
    "\n",
    "#print(pm6y6_high_2_a_CCD100)\n",
    "#t_energies,t_stitched = sample_stitch('q','Intensity',pm6y6_low_12_a_CCD0,pm6y6_low_12_a_CCD0_energies,pm6y6_low_12_a_CCD100,pm6y6_low_12_a_CCD100_energies,smoothing=10)\n",
    "#plot_selectenergy('TEST',t_stitched,t_energies,[250.0,282.5,284.8,286.5,400.0])\n",
    "#plot_selectenergy(PM6-Y6_HIGH_1-2_A_CCD0[0],PM6-Y6_HIGH_1-2_A_CCD0_energies[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f925b",
   "metadata": {},
   "source": [
    "## FIGURE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Function is meant as an OVERVIEW summary data processing technique, so most functions called within use default values. If you wish to customize fit parameters either update this function,    or simply copy the loop structure contained within and customize it in working code\n",
      "Code has also not been optimized to account for differing sorting order when importing USE AT OWN RISK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5027053bb2493398f26767b2420001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Session Progress:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c26c769c9b49d0b6693cc2153601fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acae38846944c21a580ba4300f53a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a518c4ba2b0a42b28c1960e04b2d5162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a43fda4f58469b9995db99020b0004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3629f765d92147279407885beef9ef0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57811ddb94df436e978bb86b404c391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e4b19652d34551a1a0f6616e3b61cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''#Working Session export code)\n",
    "#session_stitch(sample_data[:2],sample_energies[:2],[280.0,282.0,284.2,285.5],285.1)\n",
    "#sample_stitch('q','Intensity',Ace_2_A_CCD0,Ace_2_A_CCD0_energies,Ace_2_A_CCD100,Ace_2_A_CCD100_energies)\n",
    "#print(len(Ace_0p5_A_CCD0))\n",
    "#print(len(Ace_0p5_A_CCD100))\n",
    "#print(PM6_B_CCD100_energies)\n",
    "#print(len(Ace_0p5_A_CCD100_energies))\n",
    "session_stitch(session_data,session_energies,[280.0,282.5,284.8,286.5],285.1,f_smooth=25,f_suffix=\"180\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCD100 Check\n",
      "[269.9, 280.0, 281.0, 282.5, 282.0, 283.2, 283.5, 283.0, 284.2, 284.5, 284.8, 284.0, 285.2, 285.4, 285.5, 285.8, 285.0, 286.5, 286.0, 287.5, 287.0, 288.5, 288.0, 289.5, 289.0, 290.0, 295.0, 300.0, 310.0, 320.0]\n",
      "30\n",
      "30\n",
      "CCD0 Check\n",
      "[270.0, 280.0, 281.0, 282.5, 282.0, 283.2, 283.5, 283.0, 284.2, 284.5, 284.8, 284.0, 285.2, 285.4, 285.5, 285.8, 285.0, 286.5, 286.0, 287.5, 287.0, 288.5, 288.0, 289.5, 289.0, 290.0, 295.0, 300.0, 310.0, 320.0]\n",
      "30\n",
      "30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8520726a94f228a6ab712b977728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sample In Progress:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmax' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Developer/Scattering/All in One/Scattering_Analysis_Functions.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(SW_2F_B_CCD0))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#print(SW_2F_B_CCD0[0])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m sample_stitch(\u001b[39m'\u001b[39;49m\u001b[39mq\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mIntensity\u001b[39;49m\u001b[39m'\u001b[39;49m,SW_2F_B_CCD0,SW_2F_B_CCD0_energies,SW_2F_B_CCD100,SW_2F_B_CCD100_energies)\n",
      "\u001b[1;32m/Users/stephen/Developer/Scattering/All in One/Scattering_Analysis_Functions.ipynb Cell 45\u001b[0m in \u001b[0;36msample_stitch\u001b[0;34m(axisX, axisY, ccd0_data, ccd0_energy, ccd100_data, ccd100_energy, q, smoothing, window, regime, method)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#Progress Bar:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#bar1 = tqdm(total=len(energies), position=0, dynamic_ncols=True, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#Loop to Access Each Dataframe in the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(energies)),desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSample In Progress\u001b[39m\u001b[39m\"\u001b[39m,colour\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#for index in tqdm(range(len(energies)), position=0, leave=True, unit='dataframe', desc=\"Stitching In Progress\", bar_format='{l_bar}{bar:50}| {n_fmt}/{total_fmt} [{elapsed}]'):\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     temp_df \u001b[39m=\u001b[39m analysis_stitch(axisX,axisY,ccd0_data[index],ccd100_data[index],q,smoothing,window,regime)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     stitched\u001b[39m.\u001b[39mappend(temp_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#bar1.update(int(1))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m#print(str(index+1)+\"/\"+str(len(energies))+\" dataframes complete\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m#print()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m#Stich the dataframes into the new list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#Return the new stuff as tuples\u001b[39;00m\n",
      "\u001b[1;32m/Users/stephen/Developer/Scattering/All in One/Scattering_Analysis_Functions.ipynb Cell 45\u001b[0m in \u001b[0;36manalysis_stitch\u001b[0;34m(axisX, axisY, ccd100, ccd0, q, smoothing, window, regime, method, debug, checkpoint, progress)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     ccd0\u001b[39m.\u001b[39mplot(x \u001b[39m=\u001b[39maxisX, y\u001b[39m=\u001b[39maxisY, kind \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mline\u001b[39m\u001b[39m'\u001b[39m,ax\u001b[39m=\u001b[39max1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#Instead of trimming the data sets, identify the nearest indices in both dataframes\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#sAssumes increase in the q of the X Axis\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m index_max \u001b[39m=\u001b[39m ccd0working[axisX]\u001b[39m.\u001b[39;49midxmax() \u001b[39m#I think this value is trivial...\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m q_max \u001b[39m=\u001b[39m ccd0working[axisX]\u001b[39m.\u001b[39mmax()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Developer/Scattering/All%20in%20One/Scattering_Analysis_Functions.ipynb#X62sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m index_min \u001b[39m=\u001b[39m ccd100working[axisX]\u001b[39m.\u001b[39midxmin() \u001b[39m#I think this value is trivial...\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/miniconda3/lib/python3.9/site-packages/pandas/core/series.py:2404\u001b[0m, in \u001b[0;36mSeries.idxmax\u001b[0;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39midxmax\u001b[39m(\u001b[39mself\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2340\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2341\u001b[0m \u001b[39m    Return the row label of the maximum value.\u001b[39;00m\n\u001b[1;32m   2342\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[39m    nan\u001b[39;00m\n\u001b[1;32m   2403\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margmax(axis, skipna, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2405\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m   2406\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mnan\n",
      "File \u001b[0;32m~/Developer/miniconda3/lib/python3.9/site-packages/pandas/core/base.py:657\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmax\u001b[0;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[39mreturn\u001b[39;00m delegate\u001b[39m.\u001b[39margmax()\n\u001b[1;32m    654\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[39m# error: Incompatible return value type (got \"Union[int, ndarray]\", expected\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[39m# \"int\")\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m     \u001b[39mreturn\u001b[39;00m nanops\u001b[39m.\u001b[39;49mnanargmax(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    658\u001b[0m         delegate, skipna\u001b[39m=\u001b[39;49mskipna\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/miniconda3/lib/python3.9/site-packages/pandas/core/nanops.py:88\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck(obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m obj_iter):\n\u001b[1;32m     87\u001b[0m     f_name \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreduction operation \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not allowed for this dtype\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: reduction operation 'argmax' not allowed for this dtype"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07494c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
